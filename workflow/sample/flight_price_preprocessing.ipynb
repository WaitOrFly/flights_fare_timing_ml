{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flight Price ML - ë°ì´í„° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì€ í•­ê³µê¶Œ ê°€ê²© ì˜ˆì¸¡ì„ ìœ„í•œ ì „ì²´ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ì„ í¬í•¨í•©ë‹ˆë‹¤:\n",
    "\n",
    "1. **ë°ì´í„° ë¡œë”© ë° íƒìƒ‰**\n",
    "2. **ë°ì´í„° ì •ì œ (ì¤‘ë³µ ì œê±°)**\n",
    "3. **Outlier ë¶„ì„ ë° ì²˜ë¦¬**\n",
    "4. **Feature Engineering**: ì›ë³¸ ë°ì´í„° â†’ Feature Schema í˜•ì‹\n",
    "5. **ML Preprocessing**: Feature â†’ ëª¨ë¸ ì…ë ¥ í˜•ì‹ (ì¸ì½”ë”©/ìŠ¤ì¼€ì¼ë§)\n",
    "6. **ë°ì´í„° ì €ì¥ ë° ê²€ì¦**\n",
    "\n",
    "## Feature Schema\n",
    "- `purchase_day_of_week`: êµ¬ë§¤ ìš”ì¼ (one-hot)\n",
    "- `purchase_time_bucket`: êµ¬ë§¤ ì‹œê°„ëŒ€ (one-hot)\n",
    "- `days_until_departure_bucket`: ì¶œë°œê¹Œì§€ ë‚¨ì€ ì¼ìˆ˜ (ordinal)\n",
    "- `is_weekend_departure`: ì£¼ë§ ì¶œë°œ ì—¬ë¶€ (boolean)\n",
    "- `is_holiday_season`: íœ´ê°€ ì„±ìˆ˜ê¸° ì—¬ë¶€ (boolean)\n",
    "- `price_trend_7d`: ê°€ê²© ì¶”ì„¸ (numeric)\n",
    "- `current_vs_historical_avg`: í˜„ì¬/í‰ê·  ê°€ê²© ë¹„ìœ¨ (numeric)\n",
    "- `route_hash`: ê²½ë¡œ í•´ì‹œ (categorical)\n",
    "- `stops_count`: ê²½ìœ  íšŸìˆ˜ (numeric)\n",
    "- `flight_duration_bucket`: ë¹„í–‰ ì‹œê°„ êµ¬ê°„ (one-hot)\n",
    "- **Target**: `price`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. í™˜ê²½ ì„¤ì • ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import hashlib\n",
    "import warnings\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(\"âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ë°ì´í„° ë¡œë”© ë° íƒìƒ‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„° ë¡œë“œ\n",
    "df_raw = pd.read_csv('data/raw/flight_fares.csv')\n",
    "\n",
    "print(f\"ë°ì´í„°ì…‹ í¬ê¸°: {df_raw.shape}\")\n",
    "print(f\"\\nì»¬ëŸ¼ ëª©ë¡:\")\n",
    "print(df_raw.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„° íƒ€ì… ë° ê²°ì¸¡ì¹˜ í™•ì¸\n",
    "print(\"ë°ì´í„° íƒ€ì…:\")\n",
    "print(df_raw.dtypes)\n",
    "print(\"\\nê²°ì¸¡ì¹˜:\")\n",
    "print(df_raw.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê¸°ë³¸ í†µê³„\n",
    "df_raw.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì¤‘ë³µ ë°ì´í„° í™•ì¸\n",
    "duplicates = df_raw.duplicated().sum()\n",
    "duplicate_pct = (duplicates / len(df_raw)) * 100\n",
    "\n",
    "print(f\"ğŸ” ì¤‘ë³µ ë°ì´í„° ë¶„ì„:\")\n",
    "print(f\"  - ì´ ë°ì´í„°: {len(df_raw):,}ê°œ\")\n",
    "print(f\"  - ì¤‘ë³µ ë°ì´í„°: {duplicates:,}ê°œ\")\n",
    "print(f\"  - ì¤‘ë³µ ë¹„ìœ¨: {duplicate_pct:.2f}%\")\n",
    "\n",
    "if duplicates > 0:\n",
    "    print(f\"\\nâš ï¸ {duplicates:,}ê°œì˜ ì¤‘ë³µ ë°ì´í„°ê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤. ë‹¤ìŒ ë‹¨ê³„ì—ì„œ ì œê±°í•©ë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê°€ê²© ë¶„í¬ ì‹œê°í™” (ì´ˆê¸°)\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Histogram\n",
    "axes[0, 0].hist(df_raw['Fare'], bins=50, edgecolor='black')\n",
    "axes[0, 0].set_title('Fare Distribution (Original)')\n",
    "axes[0, 0].set_xlabel('Fare')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "\n",
    "# Box plot\n",
    "axes[0, 1].boxplot(df_raw['Fare'])\n",
    "axes[0, 1].set_title('Fare Box Plot')\n",
    "axes[0, 1].set_ylabel('Fare')\n",
    "\n",
    "# Log scale histogram\n",
    "axes[1, 0].hist(np.log1p(df_raw['Fare']), bins=50, edgecolor='black')\n",
    "axes[1, 0].set_title('Log(Fare) Distribution')\n",
    "axes[1, 0].set_xlabel('Log(Fare)')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "\n",
    "# Q-Q plot\n",
    "stats.probplot(df_raw['Fare'], dist=\"norm\", plot=axes[1, 1])\n",
    "axes[1, 1].set_title('Q-Q Plot (Normal Distribution)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Skewness ê³„ì‚°\n",
    "skewness = df_raw['Fare'].skew()\n",
    "print(f\"\\nğŸ“Š ê°€ê²© í†µê³„:\")\n",
    "print(f\"  - Skewness: {skewness:.2f}\")\n",
    "if abs(skewness) > 1:\n",
    "    print(f\"  âš ï¸ Highly skewed (|skew| > 1)\")\n",
    "elif abs(skewness) > 0.5:\n",
    "    print(f\"  âš ï¸ Moderately skewed (|skew| > 0.5)\")\n",
    "print(f\"\\n{df_raw['Fare'].describe()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ë°ì´í„° ì •ì œ (ì¤‘ë³µ ì œê±°)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì¤‘ë³µ ë°ì´í„° ì œê±°\n",
    "df_raw_before = df_raw.shape[0]\n",
    "df_raw = df_raw.drop_duplicates()\n",
    "df_raw_after = df_raw.shape[0]\n",
    "removed = df_raw_before - df_raw_after\n",
    "\n",
    "print(f\"âœ… ì¤‘ë³µ ì œê±° ì™„ë£Œ\")\n",
    "print(f\"  - ì œê±° ì „: {df_raw_before:,}ê°œ\")\n",
    "print(f\"  - ì œê±° í›„: {df_raw_after:,}ê°œ\")\n",
    "print(f\"  - ì œê±°ëœ ë°ì´í„°: {removed:,}ê°œ ({(removed/df_raw_before)*100:.2f}%)\")\n",
    "\n",
    "# ì¸ë±ìŠ¤ ë¦¬ì…‹\n",
    "df_raw = df_raw.reset_index(drop=True)\n",
    "print(f\"\\nìµœì¢… ë°ì´í„°ì…‹ í¬ê¸°: {df_raw.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Outlier ë¶„ì„ ë° ì²˜ë¦¬\n",
    "\n",
    "ê°€ê²© ë°ì´í„°ëŠ” skewedë˜ì–´ ìˆê³  outlierê°€ ë§ìŠµë‹ˆë‹¤. ì—¬ëŸ¬ ì²˜ë¦¬ ë°©ë²•ì„ ì œê³µí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_outliers_iqr(data, column, multiplier=1.5):\n",
    "    \"\"\"\n",
    "    IQR ë°©ë²•ìœ¼ë¡œ outlier íƒì§€\n",
    "    \n",
    "    Args:\n",
    "        data: DataFrame\n",
    "        column: ì»¬ëŸ¼ëª…\n",
    "        multiplier: IQR ë°°ìˆ˜ (ê¸°ë³¸ 1.5, ë” ì—„ê²©í•˜ê²ŒëŠ” 3.0)\n",
    "    \n",
    "    Returns:\n",
    "        lower_bound, upper_bound, outlier_mask\n",
    "    \"\"\"\n",
    "    Q1 = data[column].quantile(0.25)\n",
    "    Q3 = data[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    lower_bound = Q1 - multiplier * IQR\n",
    "    upper_bound = Q3 + multiplier * IQR\n",
    "    \n",
    "    outlier_mask = (data[column] < lower_bound) | (data[column] > upper_bound)\n",
    "    \n",
    "    return lower_bound, upper_bound, outlier_mask\n",
    "\n",
    "# Outlier íƒì§€\n",
    "lower, upper, outlier_mask = detect_outliers_iqr(df_raw, 'Fare', multiplier=1.5)\n",
    "n_outliers = outlier_mask.sum()\n",
    "outlier_pct = (n_outliers / len(df_raw)) * 100\n",
    "\n",
    "print(f\"ğŸ” Outlier ë¶„ì„ (IQR ë°©ë²•, multiplier=1.5):\")\n",
    "print(f\"  - Lower bound: â‚¹{lower:,.0f}\")\n",
    "print(f\"  - Upper bound: â‚¹{upper:,.0f}\")\n",
    "print(f\"  - Outliers: {n_outliers:,}ê°œ ({outlier_pct:.2f}%)\")\n",
    "print(f\"  - Min outlier: â‚¹{df_raw[outlier_mask]['Fare'].min():,.0f}\")\n",
    "print(f\"  - Max outlier: â‚¹{df_raw[outlier_mask]['Fare'].max():,.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‹œê°í™”: Outlier ë¶„í¬\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Scatter plot\n",
    "axes[0].scatter(range(len(df_raw)), df_raw['Fare'], \n",
    "                c=outlier_mask, cmap='coolwarm', alpha=0.5, s=1)\n",
    "axes[0].axhline(y=upper, color='r', linestyle='--', label=f'Upper bound: â‚¹{upper:,.0f}')\n",
    "axes[0].axhline(y=lower, color='r', linestyle='--', label=f'Lower bound: â‚¹{lower:,.0f}')\n",
    "axes[0].set_title('Fare Distribution with Outliers')\n",
    "axes[0].set_xlabel('Index')\n",
    "axes[0].set_ylabel('Fare')\n",
    "axes[0].legend()\n",
    "\n",
    "# Violin plot\n",
    "axes[1].violinplot([df_raw[~outlier_mask]['Fare'], df_raw[outlier_mask]['Fare']], \n",
    "                    positions=[1, 2], showmeans=True)\n",
    "axes[1].set_xticks([1, 2])\n",
    "axes[1].set_xticklabels(['Normal', 'Outliers'])\n",
    "axes[1].set_ylabel('Fare')\n",
    "axes[1].set_title('Normal vs Outliers Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier ì²˜ë¦¬ ë°©ë²• ì„ íƒ\n",
    "\n",
    "ì•„ë˜ 3ê°€ì§€ ì˜µì…˜ ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•˜ì„¸ìš”:\n",
    "\n",
    "**ì˜µì…˜ 1: Outlier ì œê±°** - ê·¹ë‹¨ê°’ ì™„ì „ ì œê±°  \n",
    "**ì˜µì…˜ 2: Outlier Clipping (Winsorization)** - ê·¹ë‹¨ê°’ì„ ê²½ê³„ê°’ìœ¼ë¡œ ëŒ€ì²´ (ê¶Œì¥)  \n",
    "**ì˜µì…˜ 3: Log Transformation** - ê°€ê²©ì— log ë³€í™˜ ì ìš© (skewness ê°ì†Œ)\n",
    "\n",
    "ì•„ë˜ì—ì„œ í•˜ë‚˜ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì˜µì…˜ ì„¤ì •\n",
    "OUTLIER_METHOD = 'clip'  # 'remove', 'clip', 'log', 'none' ì¤‘ ì„ íƒ\n",
    "\n",
    "df_processed = df_raw.copy()\n",
    "\n",
    "if OUTLIER_METHOD == 'remove':\n",
    "    # ì˜µì…˜ 1: Outlier ì œê±°\n",
    "    before = len(df_processed)\n",
    "    df_processed = df_processed[~outlier_mask].reset_index(drop=True)\n",
    "    after = len(df_processed)\n",
    "    print(f\"âœ… Outlier ì œê±° ì™„ë£Œ\")\n",
    "    print(f\"  - ì œê±° ì „: {before:,}ê°œ\")\n",
    "    print(f\"  - ì œê±° í›„: {after:,}ê°œ\")\n",
    "    print(f\"  - ì œê±°ëœ ë°ì´í„°: {before - after:,}ê°œ ({((before-after)/before)*100:.2f}%)\")\n",
    "    \n",
    "elif OUTLIER_METHOD == 'clip':\n",
    "    # ì˜µì…˜ 2: Outlier Clipping (ê¶Œì¥)\n",
    "    df_processed['Fare'] = df_processed['Fare'].clip(lower=lower, upper=upper)\n",
    "    print(f\"âœ… Outlier Clipping ì™„ë£Œ\")\n",
    "    print(f\"  - Lower bound: â‚¹{lower:,.0f}\")\n",
    "    print(f\"  - Upper bound: â‚¹{upper:,.0f}\")\n",
    "    print(f\"  - {n_outliers:,}ê°œì˜ ê°’ì´ ê²½ê³„ê°’ìœ¼ë¡œ ëŒ€ì²´ë˜ì—ˆìŠµë‹ˆë‹¤\")\n",
    "    \n",
    "elif OUTLIER_METHOD == 'log':\n",
    "    # ì˜µì…˜ 3: Log Transformation\n",
    "    # Note: Feature engineering í›„ targetì— ì ìš©í•´ì•¼ í•¨\n",
    "    print(f\"âœ… Log Transformation ì„ íƒ\")\n",
    "    print(f\"  - Feature engineering í›„ targetì— log1p ë³€í™˜ì„ ì ìš©í•©ë‹ˆë‹¤\")\n",
    "    print(f\"  - ëª¨ë¸ ì˜ˆì¸¡ í›„ expm1ìœ¼ë¡œ ì—­ë³€í™˜ í•„ìš”\")\n",
    "    \n",
    "else:\n",
    "    # ì˜µì…˜ 4: ì²˜ë¦¬ ì•ˆ í•¨\n",
    "    print(f\"â„¹ï¸ Outlier ì²˜ë¦¬ ì—†ìŒ - ì›ë³¸ ë°ì´í„° ì‚¬ìš©\")\n",
    "\n",
    "print(f\"\\nìµœì¢… ë°ì´í„° í¬ê¸°: {df_processed.shape}\")\n",
    "print(f\"\\nì²˜ë¦¬ í›„ ê°€ê²© í†µê³„:\")\n",
    "print(df_processed['Fare'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì²˜ë¦¬ í›„ ë¶„í¬ í™•ì¸\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# ì›ë³¸ vs ì²˜ë¦¬ í›„ ë¹„êµ\n",
    "axes[0].hist(df_raw['Fare'], bins=50, alpha=0.5, label='Original', edgecolor='black')\n",
    "axes[0].hist(df_processed['Fare'], bins=50, alpha=0.5, label='Processed', edgecolor='black')\n",
    "axes[0].set_title('Fare Distribution: Before vs After')\n",
    "axes[0].set_xlabel('Fare')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].legend()\n",
    "\n",
    "# Box plot ë¹„êµ\n",
    "axes[1].boxplot([df_raw['Fare'], df_processed['Fare']], labels=['Original', 'Processed'])\n",
    "axes[1].set_title('Box Plot Comparison')\n",
    "axes[1].set_ylabel('Fare')\n",
    "\n",
    "# Skewness ê°œì„  í™•ì¸\n",
    "original_skew = df_raw['Fare'].skew()\n",
    "processed_skew = df_processed['Fare'].skew()\n",
    "axes[2].bar(['Original', 'Processed'], [original_skew, processed_skew])\n",
    "axes[2].axhline(y=0, color='r', linestyle='--')\n",
    "axes[2].set_title('Skewness Comparison')\n",
    "axes[2].set_ylabel('Skewness')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nğŸ“Š Skewness ê°œì„ :\")\n",
    "print(f\"  - Original: {original_skew:.3f}\")\n",
    "print(f\"  - Processed: {processed_skew:.3f}\")\n",
    "print(f\"  - Improvement: {((original_skew - processed_skew) / original_skew * 100):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering\n",
    "\n",
    "ì²˜ë¦¬ëœ ë°ì´í„°ë¥¼ Feature Schemaì— ë§ê²Œ ë³€í™˜í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlightFeatureEngineer:\n",
    "    \"\"\"\n",
    "    ì›ë³¸ ë°ì´í„°ë¥¼ feature schemaì— ì •ì˜ëœ featureë¡œ ë³€í™˜\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, apply_log_to_target=False):\n",
    "        # ì¸ë„ ê³µíœ´ì¼ ì‹œì¦Œ ì •ì˜\n",
    "        self.holiday_months = [1, 3, 4, 5, 6, 8, 10, 11]  # Republic Day, Holi, ì—¬ë¦„íœ´ê°€, Independence Day, Diwali/Dussehra\n",
    "        self.apply_log_to_target = apply_log_to_target\n",
    "        \n",
    "    def transform(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        ì›ë³¸ ë°ì´í„°ë¥¼ feature schema í˜•ì‹ìœ¼ë¡œ ë³€í™˜\n",
    "        \"\"\"\n",
    "        df = df.copy()\n",
    "        \n",
    "        # ë‚ ì§œ/ì‹œê°„ íŒŒì‹±\n",
    "        df['crawl_datetime'] = pd.to_datetime(df['Crawl Timestamp'], utc=True).dt.tz_localize(None)\n",
    "        df['departure_datetime'] = pd.to_datetime(df['Departure Date'] + ' ' + df['Departure Time'])\n",
    "        \n",
    "        features = pd.DataFrame()\n",
    "        \n",
    "        # 1. purchase_day_of_week: êµ¬ë§¤(í¬ë¡¤ë§) ì‹œì ì˜ ìš”ì¼\n",
    "        features['purchase_day_of_week'] = df['crawl_datetime'].dt.dayofweek\n",
    "        \n",
    "        # 2. purchase_time_bucket: êµ¬ë§¤ ì‹œê°„ëŒ€\n",
    "        features['purchase_time_bucket'] = df['crawl_datetime'].dt.hour.apply(\n",
    "            self._get_time_bucket\n",
    "        )\n",
    "        \n",
    "        # 3. days_until_departure_bucket: ì¶œë°œê¹Œì§€ ë‚¨ì€ ì¼ìˆ˜\n",
    "        days_until = (df['departure_datetime'] - df['crawl_datetime']).dt.days\n",
    "        features['days_until_departure_bucket'] = days_until.apply(\n",
    "            self._get_days_until_bucket\n",
    "        )\n",
    "        \n",
    "        # 4. is_weekend_departure: ì£¼ë§ ì¶œë°œ ì—¬ë¶€\n",
    "        features['is_weekend_departure'] = (\n",
    "            df['departure_datetime'].dt.dayofweek >= 5\n",
    "        ).astype(int)\n",
    "        \n",
    "        # 5. is_holiday_season: íœ´ê°€ ì„±ìˆ˜ê¸° ì—¬ë¶€\n",
    "        features['is_holiday_season'] = (\n",
    "            df['departure_datetime'].dt.month.isin(self.holiday_months)\n",
    "        ).astype(int)\n",
    "        \n",
    "        # 6. price_trend_7d: ê°€ê²© ì¶”ì„¸ (routeë³„ í‰ê·  ëŒ€ë¹„)\n",
    "        features['price_trend_7d'] = self._calculate_price_trend(df)\n",
    "        \n",
    "        # 7. current_vs_historical_avg: í˜„ì¬ ê°€ê²© vs í‰ê·  ê°€ê²© ë¹„ìœ¨\n",
    "        features['current_vs_historical_avg'] = self._calculate_price_ratio(df)\n",
    "        \n",
    "        # 8. route_hash: ì¶œë°œì§€-ëª©ì ì§€ í•´ì‹œ\n",
    "        features['route_hash'] = df.apply(\n",
    "            lambda row: self._hash_route(row['Source'], row['Destination']),\n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        # 9. stops_count: ê²½ìœ  íšŸìˆ˜\n",
    "        features['stops_count'] = df['Number Of Stops']\n",
    "        \n",
    "        # 10. flight_duration_bucket: ë¹„í–‰ ì‹œê°„ êµ¬ê°„\n",
    "        total_minutes = df['Total Time'].apply(self._parse_duration)\n",
    "        features['flight_duration_bucket'] = total_minutes.apply(\n",
    "            self._get_duration_bucket\n",
    "        )\n",
    "        \n",
    "        # Target: price (optional log transform)\n",
    "        if self.apply_log_to_target:\n",
    "            features['price'] = np.log1p(df['Fare'])\n",
    "            features['price_original'] = df['Fare']  # ì›ë³¸ ë³´ì¡´\n",
    "        else:\n",
    "            features['price'] = df['Fare']\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def _get_time_bucket(self, hour: int) -> str:\n",
    "        \"\"\"ì‹œê°„ì„ bucketìœ¼ë¡œ ë³€í™˜\"\"\"\n",
    "        if 0 <= hour < 6:\n",
    "            return 'dawn'\n",
    "        elif 6 <= hour < 12:\n",
    "            return 'morning'\n",
    "        elif 12 <= hour < 18:\n",
    "            return 'afternoon'\n",
    "        else:\n",
    "            return 'night'\n",
    "    \n",
    "    def _get_days_until_bucket(self, days: int) -> str:\n",
    "        \"\"\"ì¶œë°œê¹Œì§€ ë‚¨ì€ ì¼ìˆ˜ë¥¼ bucketìœ¼ë¡œ ë³€í™˜\"\"\"\n",
    "        if days < 7:\n",
    "            return 'very_close'\n",
    "        elif days < 14:\n",
    "            return 'close'\n",
    "        elif days < 30:\n",
    "            return 'medium'\n",
    "        else:\n",
    "            return 'far'\n",
    "    \n",
    "    def _hash_route(self, source: str, destination: str) -> str:\n",
    "        \"\"\"ì¶œë°œì§€-ëª©ì ì§€ë¥¼ í•´ì‹œê°’ìœ¼ë¡œ ë³€í™˜\"\"\"\n",
    "        route_str = f\"{source}_{destination}\"\n",
    "        return hashlib.md5(route_str.encode()).hexdigest()[:8]\n",
    "    \n",
    "    def _parse_duration(self, duration_str: str) -> int:\n",
    "        \"\"\"\n",
    "        ë¹„í–‰ ì‹œê°„ ë¬¸ìì—´ì„ ë¶„ìœ¼ë¡œ ë³€í™˜\n",
    "        ì˜ˆ: \"2h 30m\" -> 150\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if pd.isna(duration_str):\n",
    "                return 0\n",
    "            \n",
    "            hours = 0\n",
    "            minutes = 0\n",
    "            \n",
    "            if 'h' in str(duration_str):\n",
    "                parts = str(duration_str).split('h')\n",
    "                hours = int(parts[0].strip())\n",
    "                if len(parts) > 1 and 'm' in parts[1]:\n",
    "                    minutes = int(parts[1].replace('m', '').strip())\n",
    "            elif 'm' in str(duration_str):\n",
    "                minutes = int(str(duration_str).replace('m', '').strip())\n",
    "            \n",
    "            return hours * 60 + minutes\n",
    "        except:\n",
    "            return 0\n",
    "    \n",
    "    def _get_duration_bucket(self, minutes: int) -> str:\n",
    "        \"\"\"ë¹„í–‰ ì‹œê°„ì„ bucketìœ¼ë¡œ ë³€í™˜\"\"\"\n",
    "        if minutes < 120:  # 2ì‹œê°„ ë¯¸ë§Œ\n",
    "            return 'short'\n",
    "        elif minutes < 360:  # 6ì‹œê°„ ë¯¸ë§Œ\n",
    "            return 'medium'\n",
    "        else:\n",
    "            return 'long'\n",
    "    \n",
    "    def _calculate_price_trend(self, df: pd.DataFrame) -> pd.Series:\n",
    "        \"\"\"ê°€ê²© ì¶”ì„¸ ê³„ì‚° (routeë³„ í‰ê·  ëŒ€ë¹„ ë³€í™”ìœ¨)\"\"\"\n",
    "        df['route'] = df['Source'] + '_' + df['Destination']\n",
    "        route_avg = df.groupby('route')['Fare'].transform('mean')\n",
    "        trend = (df['Fare'] - route_avg) / route_avg\n",
    "        return trend.fillna(0)\n",
    "    \n",
    "    def _calculate_price_ratio(self, df: pd.DataFrame) -> pd.Series:\n",
    "        \"\"\"í˜„ì¬ ê°€ê²© vs í‰ê·  ê°€ê²© ë¹„ìœ¨\"\"\"\n",
    "        df['route'] = df['Source'] + '_' + df['Destination']\n",
    "        route_avg = df.groupby('route')['Fare'].transform('mean')\n",
    "        ratio = df['Fare'] / route_avg\n",
    "        return ratio.fillna(1.0)\n",
    "\n",
    "print(\"âœ… FlightFeatureEngineer í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering ì‹¤í–‰\n",
    "# apply_log_to_target=Trueë¡œ ì„¤ì •í•˜ë©´ ê°€ê²©ì— log ë³€í™˜ ì ìš©\n",
    "engineer = FlightFeatureEngineer(apply_log_to_target=(OUTLIER_METHOD == 'log'))\n",
    "df_features = engineer.transform(df_processed)\n",
    "\n",
    "print(f\"âœ… Feature Engineering ì™„ë£Œ\")\n",
    "print(f\"ë³€í™˜ëœ Feature ê°œìˆ˜: {df_features.shape[1]}\")\n",
    "print(f\"\\nFeature ëª©ë¡:\")\n",
    "print(df_features.columns.tolist())\n",
    "\n",
    "if OUTLIER_METHOD == 'log':\n",
    "    print(f\"\\nâš ï¸ Targetì— log ë³€í™˜ì´ ì ìš©ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "    print(f\"   ëª¨ë¸ ì˜ˆì¸¡ í›„ np.expm1()ìœ¼ë¡œ ì—­ë³€í™˜í•´ì•¼ í•©ë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë³€í™˜ëœ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°\n",
    "df_features.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering í›„ ì¤‘ë³µ í™•ì¸\n",
    "duplicates_after_fe = df_features.duplicated().sum()\n",
    "duplicate_pct_fe = (duplicates_after_fe / len(df_features)) * 100\n",
    "\n",
    "print(f\"ğŸ” Feature Engineering í›„ ì¤‘ë³µ ë°ì´í„° ë¶„ì„:\")\n",
    "print(f\"  - ì´ ë°ì´í„°: {len(df_features):,}ê°œ\")\n",
    "print(f\"  - ì¤‘ë³µ ë°ì´í„°: {duplicates_after_fe:,}ê°œ\")\n",
    "print(f\"  - ì¤‘ë³µ ë¹„ìœ¨: {duplicate_pct_fe:.2f}%\")\n",
    "\n",
    "if duplicates_after_fe > 0:\n",
    "    print(f\"\\nâš ï¸ {duplicates_after_fe:,}ê°œì˜ ì¤‘ë³µ ë°ì´í„°ê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering í›„ ì¤‘ë³µ ì œê±°\n",
    "if duplicates_after_fe > 0:\n",
    "    df_features_before = df_features.shape[0]\n",
    "    df_features = df_features.drop_duplicates()\n",
    "    df_features = df_features.reset_index(drop=True)\n",
    "    df_features_after = df_features.shape[0]\n",
    "    removed_fe = df_features_before - df_features_after\n",
    "    \n",
    "    print(f\"âœ… Feature Engineering í›„ ì¤‘ë³µ ì œê±° ì™„ë£Œ\")\n",
    "    print(f\"  - ì œê±° ì „: {df_features_before:,}ê°œ\")\n",
    "    print(f\"  - ì œê±° í›„: {df_features_after:,}ê°œ\")\n",
    "    print(f\"  - ì œê±°ëœ ë°ì´í„°: {removed_fe:,}ê°œ ({(removed_fe/df_features_before)*100:.2f}%)\")\n",
    "else:\n",
    "    print(\"âœ… ì¤‘ë³µ ì—†ìŒ\")\n",
    "\n",
    "print(f\"\\nìµœì¢… Feature ë°ì´í„°ì…‹ í¬ê¸°: {df_features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature ë¶„í¬ í™•ì¸\n",
    "print(\"Feature í†µê³„:\")\n",
    "print(df_features.describe())\n",
    "\n",
    "print(\"\\nê²°ì¸¡ì¹˜ í™•ì¸:\")\n",
    "print(df_features.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì¹´í…Œê³ ë¦¬ feature ë¶„í¬ í™•ì¸\n",
    "categorical_features = ['purchase_time_bucket', 'days_until_departure_bucket', 'flight_duration_bucket']\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, col in enumerate(categorical_features):\n",
    "    df_features[col].value_counts().plot(kind='bar', ax=axes[idx])\n",
    "    axes[idx].set_title(f'{col} Distribution')\n",
    "    axes[idx].set_xlabel(col)\n",
    "    axes[idx].set_ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train/Validation/Test Split (70/10/20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Targetê³¼ features ë¶„ë¦¬\n",
    "target_col = 'price'\n",
    "X = df_features.drop([c for c in df_features.columns if 'price' in c], axis=1)\n",
    "y = df_features[target_col]\n",
    "\n",
    "# ë¨¼ì € Train(70%) / Temp(30%) ë¶„í• \n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Tempë¥¼ Validation(10%) / Test(20%)ë¡œ ë¶„í• \n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=2/3, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"âœ… ë°ì´í„°ì…‹ ë¶„í•  ì™„ë£Œ (Train 70% / Validation 10% / Test 20%)\")\n",
    "print(f\"\\nğŸ“Š ë°ì´í„°ì…‹ í¬ê¸°:\")\n",
    "print(f\"  - ì „ì²´: {len(X):,}ê°œ\")\n",
    "print(f\"  - Train: {X_train.shape[0]:,}ê°œ ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"  - Validation: {X_val.shape[0]:,}ê°œ ({X_val.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"  - Test: {X_test.shape[0]:,}ê°œ ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "\n",
    "price_label = 'Log(Price)' if OUTLIER_METHOD == 'log' else 'Price'\n",
    "print(f\"\\nğŸ’° {price_label} í†µê³„:\")\n",
    "print(f\"  - Train í‰ê· : {y_train.mean():.2f}\")\n",
    "print(f\"  - Validation í‰ê· : {y_val.mean():.2f}\")\n",
    "print(f\"  - Test í‰ê· : {y_test.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ML Preprocessing (ì¸ì½”ë”© & ìŠ¤ì¼€ì¼ë§)\n",
    "\n",
    "Featureë¥¼ ML ëª¨ë¸ ì…ë ¥ í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlightPricePreprocessor:\n",
    "    \"\"\"\n",
    "    Feature schema ê¸°ë°˜ ì „ì²˜ë¦¬ í´ë˜ìŠ¤\n",
    "    - Categorical: One-hot encoding\n",
    "    - Ordinal: Label encoding\n",
    "    - Numeric: Standardization (ì„ íƒ)\n",
    "    - Boolean: 0/1 (ê·¸ëŒ€ë¡œ)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, scale_numeric: bool = True):\n",
    "        self.scale_numeric = scale_numeric\n",
    "        self.preprocessor = None\n",
    "        self.ordinal_mapping = {\n",
    "            'very_close': 0,\n",
    "            'close': 1,\n",
    "            'medium': 2,\n",
    "            'far': 3\n",
    "        }\n",
    "        self._setup_preprocessor()\n",
    "        \n",
    "    def _setup_preprocessor(self):\n",
    "        \"\"\"ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì„¤ì •\"\"\"\n",
    "        \n",
    "        # Feature ê·¸ë£¹ ì •ì˜\n",
    "        categorical_onehot_features = [\n",
    "            'purchase_day_of_week',\n",
    "            'purchase_time_bucket', \n",
    "            'flight_duration_bucket'\n",
    "        ]\n",
    "        \n",
    "        boolean_features = [\n",
    "            'is_weekend_departure',\n",
    "            'is_holiday_season'\n",
    "        ]\n",
    "        \n",
    "        numeric_features = [\n",
    "            'price_trend_7d',\n",
    "            'current_vs_historical_avg',\n",
    "            'stops_count'\n",
    "        ]\n",
    "        \n",
    "        high_cardinality_features = ['route_hash']\n",
    "        \n",
    "        # ColumnTransformer êµ¬ì„±\n",
    "        transformers = []\n",
    "        \n",
    "        # 1. Categorical (one-hot)\n",
    "        transformers.append((\n",
    "            'cat_onehot',\n",
    "            OneHotEncoder(\n",
    "                drop='first',\n",
    "                sparse_output=False,\n",
    "                handle_unknown='ignore'\n",
    "            ),\n",
    "            categorical_onehot_features\n",
    "        ))\n",
    "        \n",
    "        # 2. Numeric features\n",
    "        if self.scale_numeric:\n",
    "            transformers.append((\n",
    "                'num',\n",
    "                StandardScaler(),\n",
    "                numeric_features\n",
    "            ))\n",
    "        else:\n",
    "            transformers.append((\n",
    "                'num',\n",
    "                'passthrough',\n",
    "                numeric_features\n",
    "            ))\n",
    "        \n",
    "        # 3. Boolean features (ê·¸ëŒ€ë¡œ)\n",
    "        transformers.append((\n",
    "            'bool',\n",
    "            'passthrough',\n",
    "            boolean_features\n",
    "        ))\n",
    "        \n",
    "        # 4. High cardinality (ê·¸ëŒ€ë¡œ)\n",
    "        transformers.append((\n",
    "            'high_card',\n",
    "            'passthrough',\n",
    "            high_cardinality_features\n",
    "        ))\n",
    "        \n",
    "        self.preprocessor = ColumnTransformer(\n",
    "            transformers=transformers,\n",
    "            remainder='drop'\n",
    "        )\n",
    "        \n",
    "    def _encode_ordinal_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Ordinal feature ì¸ì½”ë”©\"\"\"\n",
    "        df = df.copy()\n",
    "        if 'days_until_departure_bucket' in df.columns:\n",
    "            df['days_until_departure_bucket'] = df['days_until_departure_bucket'].map(\n",
    "                self.ordinal_mapping\n",
    "            )\n",
    "        return df\n",
    "    \n",
    "    def fit(self, X: pd.DataFrame, y=None):\n",
    "        \"\"\"ì „ì²˜ë¦¬ê¸°ë¥¼ í•™ìŠµ ë°ì´í„°ì— fit\"\"\"\n",
    "        X_processed = self._encode_ordinal_features(X)\n",
    "        self.preprocessor.fit(X_processed)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X: pd.DataFrame) -> np.ndarray:\n",
    "        \"\"\"ë°ì´í„°ë¥¼ ì „ì²˜ë¦¬í•˜ì—¬ ë³€í™˜\"\"\"\n",
    "        X_processed = self._encode_ordinal_features(X)\n",
    "        X_transformed = self.preprocessor.transform(X_processed)\n",
    "        return X_transformed\n",
    "    \n",
    "    def fit_transform(self, X: pd.DataFrame, y=None) -> np.ndarray:\n",
    "        \"\"\"fitê³¼ transformì„ í•œ ë²ˆì— ìˆ˜í–‰\"\"\"\n",
    "        return self.fit(X, y).transform(X)\n",
    "    \n",
    "    def get_feature_names_out(self):\n",
    "        \"\"\"ë³€í™˜ í›„ feature ì´ë¦„ ë°˜í™˜\"\"\"\n",
    "        try:\n",
    "            return list(self.preprocessor.get_feature_names_out())\n",
    "        except AttributeError:\n",
    "            return ['feature_' + str(i) for i in range(self.preprocessor.transform(X_train.head(1)).shape[1])]\n",
    "\n",
    "print(\"âœ… FlightPricePreprocessor í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing ì‹¤í–‰\n",
    "preprocessor = FlightPricePreprocessor(scale_numeric=True)\n",
    "\n",
    "# Train ë°ì´í„°ë¡œ fit & transform\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "\n",
    "# Validation & Test ë°ì´í„° transform\n",
    "X_val_processed = preprocessor.transform(X_val)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "print(f\"âœ… ML Preprocessing ì™„ë£Œ\")\n",
    "print(f\"ì›ë³¸ feature ìˆ˜: {X_train.shape[1]}\")\n",
    "print(f\"ë³€í™˜ í›„ feature ìˆ˜: {X_train_processed.shape[1]}\")\n",
    "print(f\"\\nğŸ“Š ë°ì´í„°ì…‹ í¬ê¸°:\")\n",
    "print(f\"  - Train: {X_train_processed.shape}\")\n",
    "print(f\"  - Validation: {X_val_processed.shape}\")\n",
    "print(f\"  - Test: {X_test_processed.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë³€í™˜ëœ feature ì´ë¦„ í™•ì¸\n",
    "feature_names = preprocessor.get_feature_names_out()\n",
    "print(f\"Feature ì´ë¦„ ({len(feature_names)}ê°œ):\")\n",
    "for i, name in enumerate(feature_names, 1):\n",
    "    print(f\"{i:2d}. {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë³€í™˜ëœ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°\n",
    "X_train_df = pd.DataFrame(X_train_processed, columns=feature_names)\n",
    "X_train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ë°ì´í„° ì €ì¥\n",
    "\n",
    "ì „ì²˜ë¦¬ëœ ë°ì´í„°ë¥¼ ì €ì¥í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# ë””ë ‰í† ë¦¬ ìƒì„±\n",
    "os.makedirs('data/processed', exist_ok=True)\n",
    "\n",
    "# Feature engineered ë°ì´í„° ì €ì¥ (CSV)\n",
    "df_features.to_csv('data/processed/features.csv', index=False)\n",
    "print(\"âœ… Feature engineered data ì €ì¥ ì™„ë£Œ: data/processed/features.csv\")\n",
    "\n",
    "# ì „ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥ (numpy)\n",
    "np.save('data/processed/X_train.npy', X_train_processed)\n",
    "np.save('data/processed/X_val.npy', X_val_processed)\n",
    "np.save('data/processed/X_test.npy', X_test_processed)\n",
    "np.save('data/processed/y_train.npy', y_train.values)\n",
    "np.save('data/processed/y_val.npy', y_val.values)\n",
    "np.save('data/processed/y_test.npy', y_test.values)\n",
    "print(\"âœ… Preprocessed data ì €ì¥ ì™„ë£Œ: data/processed/*.npy\")\n",
    "\n",
    "# Feature ì´ë¦„ ì €ì¥\n",
    "with open('data/processed/feature_names.txt', 'w') as f:\n",
    "    for name in feature_names:\n",
    "        f.write(name + '\\n')\n",
    "print(\"âœ… Feature names ì €ì¥ ì™„ë£Œ: data/processed/feature_names.txt\")\n",
    "\n",
    "# ë©”íƒ€ë°ì´í„° ì €ì¥ (outlier ì²˜ë¦¬ ë°©ë²• ë“±)\n",
    "metadata = {\n",
    "    'outlier_method': OUTLIER_METHOD,\n",
    "    'log_transformed': (OUTLIER_METHOD == 'log'),\n",
    "    'outlier_bounds': {'lower': float(lower), 'upper': float(upper)}\n",
    "}\n",
    "import json\n",
    "with open('data/processed/metadata.json', 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "print(\"âœ… Metadata ì €ì¥ ì™„ë£Œ: data/processed/metadata.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. ìš”ì•½ ë° ê²€ì¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nğŸ“Š ë°ì´í„° ì •ì œ:\")\n",
    "print(f\"  - ì¤‘ë³µ ì œê±°: {removed:,}ê°œ\")\n",
    "print(f\"  - Outlier ì²˜ë¦¬: {OUTLIER_METHOD}\")\n",
    "if OUTLIER_METHOD == 'clip':\n",
    "    print(f\"    â”” Clipped: {n_outliers:,}ê°œ\")\n",
    "elif OUTLIER_METHOD == 'remove':\n",
    "    print(f\"    â”” Removed: {n_outliers:,}ê°œ\")\n",
    "print(f\"  - Skewness: {original_skew:.2f} â†’ {processed_skew:.2f}\")\n",
    "\n",
    "print(f\"\\nğŸ“Š ë°ì´í„°ì…‹ ì •ë³´:\")\n",
    "print(f\"  - ìµœì¢… ë°ì´í„°: {len(df_processed):,}ê°œ\")\n",
    "print(f\"  - Feature engineered: {df_features.shape}\")\n",
    "print(f\"  - Train set: {X_train_processed.shape}\")\n",
    "print(f\"  - Validation set: {X_val_processed.shape}\")\n",
    "print(f\"  - Test set: {X_test_processed.shape}\")\n",
    "\n",
    "print(f\"\\nğŸ¯ Feature ì •ë³´:\")\n",
    "print(f\"  - ì›ë³¸ feature ìˆ˜: {X_train.shape[1]}\")\n",
    "print(f\"  - One-hot encoding í›„: {X_train_processed.shape[1]}\")\n",
    "\n",
    "print(f\"\\nğŸ’° Target (ê°€ê²©) ì •ë³´:\")\n",
    "if OUTLIER_METHOD == 'log':\n",
    "    print(f\"  âš ï¸ Log-transformed target\")\n",
    "    print(f\"  - Train í‰ê·  (log): {y_train.mean():.2f}\")\n",
    "    print(f\"  - Val í‰ê·  (log): {y_val.mean():.2f}\")\n",
    "    print(f\"  - Test í‰ê·  (log): {y_test.mean():.2f}\")\n",
    "else:\n",
    "    print(f\"  - Train í‰ê· : â‚¹{y_train.mean():,.0f}\")\n",
    "    print(f\"  - Validation í‰ê· : â‚¹{y_val.mean():,.0f}\")\n",
    "    print(f\"  - Test í‰ê· : â‚¹{y_test.mean():,.0f}\")\n",
    "    print(f\"  - ì „ì²´ ë²”ìœ„: â‚¹{y.min():,.0f} ~ â‚¹{y.max():,.0f}\")\n",
    "\n",
    "print(f\"\\nğŸ“ ì €ì¥ëœ íŒŒì¼:\")\n",
    "print(f\"  - data/processed/features.csv\")\n",
    "print(f\"  - data/processed/X_train.npy\")\n",
    "print(f\"  - data/processed/X_val.npy\")\n",
    "print(f\"  - data/processed/X_test.npy\")\n",
    "print(f\"  - data/processed/y_train.npy\")\n",
    "print(f\"  - data/processed/y_val.npy\")\n",
    "print(f\"  - data/processed/y_test.npy\")\n",
    "print(f\"  - data/processed/feature_names.txt\")\n",
    "print(f\"  - data/processed/metadata.json\")\n",
    "\n",
    "print(f\"\\nâœ… ë‹¤ìŒ ë‹¨ê³„: ëª¨ë¸ í•™ìŠµ ë° í‰ê°€\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. ë°ì´í„° ë¡œë“œ í—¬í¼ í•¨ìˆ˜\n",
    "\n",
    "ë‚˜ì¤‘ì— ë°ì´í„°ë¥¼ ì‰½ê²Œ ë¡œë“œí•  ìˆ˜ ìˆëŠ” í•¨ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_processed_data():\n",
    "    \"\"\"\n",
    "    ì €ì¥ëœ ì „ì²˜ë¦¬ ë°ì´í„° ë¡œë“œ\n",
    "    \n",
    "    Returns:\n",
    "        X_train, X_val, X_test, y_train, y_val, y_test, feature_names, metadata\n",
    "    \"\"\"\n",
    "    import json\n",
    "    \n",
    "    X_train = np.load('data/processed/X_train.npy')\n",
    "    X_val = np.load('data/processed/X_val.npy')\n",
    "    X_test = np.load('data/processed/X_test.npy')\n",
    "    y_train = np.load('data/processed/y_train.npy')\n",
    "    y_val = np.load('data/processed/y_val.npy')\n",
    "    y_test = np.load('data/processed/y_test.npy')\n",
    "    \n",
    "    with open('data/processed/feature_names.txt', 'r') as f:\n",
    "        feature_names = [line.strip() for line in f]\n",
    "    \n",
    "    with open('data/processed/metadata.json', 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test, feature_names, metadata\n",
    "\n",
    "print(\"âœ… load_processed_data() í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")\n",
    "print(\"   ì‚¬ìš©ë²•: X_train, X_val, X_test, y_train, y_val, y_test, feature_names, metadata = load_processed_data()\")\n",
    "print(\"\\nâš ï¸ ì¤‘ìš”: Log transformationì„ ì‚¬ìš©í•œ ê²½ìš° ì˜ˆì¸¡ê°’ì„ np.expm1()ìœ¼ë¡œ ì—­ë³€í™˜í•´ì•¼ í•©ë‹ˆë‹¤!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
