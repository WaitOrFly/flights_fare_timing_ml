"""
Flight Price Preprocessing Module

ì´ ëª¨ë“ˆì€ í•­ê³µê¶Œ ê°€ê²© ì˜ˆì¸¡ ë°ì´í„° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ì„ í¬í•¨í•©ë‹ˆë‹¤.
"""
import os
import joblib

import pandas as pd
import numpy as np
from datetime import datetime
import hashlib
import re
import warnings
from scipy import stats
import boto3
from io import StringIO

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
import mlflow
import mlflow.sklearn

warnings.filterwarnings('ignore')


class FlightFeatureEngineer:
    """
    ì›ë³¸ ë°ì´í„°ë¥¼ feature schemaì— ì •ì˜ëœ featureë¡œ ë³€í™˜
    """
    
    def __init__(self, apply_log_to_target=False):
        # ì¸ë„ ê³µíœ´ì¼ ì‹œì¦Œ ì •ì˜
        self.holiday_months = [1, 3, 4, 5, 6, 8, 10, 11]  # Republic Day, Holi, ì—¬ë¦„íœ´ê°€, Independence Day, Diwali/Dussehra
        self.apply_log_to_target = apply_log_to_target
        self.ordinal_mapping = {
            'very_close': 0,
            'close': 1,
            'medium': 2,
            'far': 3,
        }
        
    def transform(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        ì›ë³¸ ë°ì´í„°ë¥¼ feature schema í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        """
        df = df.copy()

        # ìˆ«ìí˜• ì»¬ëŸ¼ ì •ì œ
        if 'Fare' in df.columns:
            df['Fare'] = pd.to_numeric(df['Fare'], errors='coerce').fillna(0)
        if 'Number Of Stops' in df.columns:
            df['Number Of Stops'] = df['Number Of Stops'].apply(self._parse_stops)
        
        # ë‚ ì§œ/ì‹œê°„ íŒŒì‹±
        df['crawl_datetime'] = pd.to_datetime(df['Crawl Timestamp'], utc=True).dt.tz_localize(None)
        df['departure_datetime'] = pd.to_datetime(df['Departure Date'] + ' ' + df['Departure Time'])
        
        features = pd.DataFrame()
        
        # 1. purchase_day_of_week: êµ¬ë§¤(í¬ë¡¤ë§) ì‹œì ì˜ ìš”ì¼
        features['purchase_day_of_week'] = df['crawl_datetime'].dt.dayofweek
        
        # 2. purchase_time_bucket: êµ¬ë§¤ ì‹œê°„ëŒ€
        features['purchase_time_bucket'] = df['crawl_datetime'].dt.hour.apply(
            self._get_time_bucket
        )
        
        # 3. days_until_departure_bucket: ì¶œë°œê¹Œì§€ ë‚¨ì€ ì¼ìˆ˜
        days_until = (df['departure_datetime'] - df['crawl_datetime']).dt.days
        days_until_bucket = days_until.apply(self._get_days_until_bucket)
        features['days_until_departure_bucket'] = days_until_bucket.map(
            self.ordinal_mapping
        )
        
        # 4. is_weekend_departure: ì£¼ë§ ì¶œë°œ ì—¬ë¶€
        features['is_weekend_departure'] = (
            df['departure_datetime'].dt.dayofweek >= 5
        ).astype(int)
        
        # 5. is_holiday_season: íœ´ê°€ ì„±ìˆ˜ê¸° ì—¬ë¶€
        features['is_holiday_season'] = (
            df['departure_datetime'].dt.month.isin(self.holiday_months)
        ).astype(int)
        
        
        
        # 8. route_hash: ì¶œë°œì§€-ëª©ì ì§€ í•´ì‹œ
        features['route_hash'] = df.apply(
            lambda row: self._hash_route(row['Source'], row['Destination']),
            axis=1
        )
        
        # 9. stops_count: ê²½ìœ  íšŸìˆ˜
        features['stops_count'] = df['Number Of Stops']
        
        # 10. flight_duration_bucket: ë¹„í–‰ ì‹œê°„ êµ¬ê°„
        total_minutes = df['Total Time'].apply(self._parse_duration)
        features['flight_duration_bucket'] = total_minutes.apply(
            self._get_duration_bucket
        )
        
        # Target: price (log transform)
        features['price'] = np.log1p(df['Fare'])
        features['price_original'] = df['Fare']  # ?? ??
        
        return features
    
    def _get_time_bucket(self, hour: int) -> str:
        """ì‹œê°„ì„ bucketìœ¼ë¡œ ë³€í™˜"""
        if 0 <= hour < 6:
            return 'dawn'
        elif 6 <= hour < 12:
            return 'morning'
        elif 12 <= hour < 18:
            return 'afternoon'
        else:
            return 'night'
    
    def _get_days_until_bucket(self, days: int) -> str:
        """ì¶œë°œê¹Œì§€ ë‚¨ì€ ì¼ìˆ˜ë¥¼ bucketìœ¼ë¡œ ë³€í™˜"""
        if days < 7:
            return 'very_close'
        elif days < 14:
            return 'close'
        elif days < 30:
            return 'medium'
        else:
            return 'far'
    
    def _hash_route(self, source: str, destination: str) -> int:
        """ì¶œë°œì§€-ëª©ì ì§€ë¥¼ í•´ì‹œê°’ìœ¼ë¡œ ë³€í™˜"""
        route_str = f"{source}_{destination}"
        return int(hashlib.md5(route_str.encode()).hexdigest()[:8], 16)

    def _parse_stops(self, value) -> int:
        """ê²½ìœ  íšŸìˆ˜ë¥¼ ì •ìˆ˜ë¡œ ë³€í™˜"""
        if pd.isna(value):
            return 0
        if isinstance(value, (int, np.integer)):
            return int(value)
        text = str(value).strip().lower()
        if "non" in text:
            return 0
        match = re.search(r"\d+", text)
        return int(match.group(0)) if match else 0
    
    def _parse_duration(self, duration_str: str) -> int:
        """
        ë¹„í–‰ ì‹œê°„ ë¬¸ìì—´ì„ ë¶„ìœ¼ë¡œ ë³€í™˜
        ì˜ˆ: "2h 30m" -> 150
        """
        try:
            if pd.isna(duration_str):
                return 0
            
            hours = 0
            minutes = 0
            
            if 'h' in str(duration_str):
                parts = str(duration_str).split('h')
                hours = int(parts[0].strip())
                if len(parts) > 1 and 'm' in parts[1]:
                    minutes = int(parts[1].replace('m', '').strip())
            elif 'm' in str(duration_str):
                minutes = int(str(duration_str).replace('m', '').strip())
            
            return hours * 60 + minutes
        except:
            return 0
    
    def _get_duration_bucket(self, minutes: int) -> str:
        """ë¹„í–‰ ì‹œê°„ì„ bucketìœ¼ë¡œ ë³€í™˜"""
        if minutes < 120:  # 2ì‹œê°„ ë¯¸ë§Œ
            return 'short'
        elif minutes < 360:  # 6ì‹œê°„ ë¯¸ë§Œ
            return 'medium'
        else:
            return 'long'
    
    def _calculate_price_trend(self, df: pd.DataFrame) -> pd.Series:
        """ê°€ê²© ì¶”ì„¸ ê³„ì‚° (routeë³„ í‰ê·  ëŒ€ë¹„ ë³€í™”ìœ¨)"""
        df['route'] = df['Source'] + '_' + df['Destination']
        route_avg = df.groupby('route')['Fare'].transform('mean')
        trend = (df['Fare'] - route_avg) / route_avg
        return trend.fillna(0)
    
    def _calculate_price_ratio(self, df: pd.DataFrame) -> pd.Series:
        """í˜„ì¬ ê°€ê²© vs í‰ê·  ê°€ê²© ë¹„ìœ¨"""
        df['route'] = df['Source'] + '_' + df['Destination']
        route_avg = df.groupby('route')['Fare'].transform('mean')
        ratio = df['Fare'] / route_avg
        return ratio.fillna(1.0)


class FlightPricePreprocessor:
    """
    Feature schema ê¸°ë°˜ ì „ì²˜ë¦¬ í´ë˜ìŠ¤
    - Categorical: One-hot encoding
    - Ordinal: Label encoding
    - Numeric: Standardization (ì„ íƒ)
    - Boolean: 0/1 (ê·¸ëŒ€ë¡œ)
    """
    
    def __init__(self, scale_numeric: bool = True):
        self.scale_numeric = scale_numeric
        self.preprocessor = None
        self.ordinal_mapping = {
            'very_close': 0,
            'close': 1,
            'medium': 2,
            'far': 3
        }
        self._setup_preprocessor()
        
    def _setup_preprocessor(self):
        """ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì„¤ì •"""
        
        # Feature ê·¸ë£¹ ì •ì˜
        categorical_onehot_features = [
            'purchase_day_of_week',
            'purchase_time_bucket', 
            'flight_duration_bucket'
        ]
        
        boolean_features = [
            'is_weekend_departure',
            'is_holiday_season'
        ]
        
        numeric_features = [
            'stops_count',
            'days_until_departure_bucket'
        ]
        
        high_cardinality_features = ['route_hash']
        
        # ColumnTransformer êµ¬ì„±
        transformers = []
        
        # 1. Categorical (one-hot)
        transformers.append((
            'cat_onehot',
            OneHotEncoder(
                drop='first',
                sparse_output=False,
                handle_unknown='ignore'
            ),
            categorical_onehot_features
        ))
        
        # 2. Numeric features
        if self.scale_numeric:
            transformers.append((
                'num',
                StandardScaler(),
                numeric_features
            ))
        else:
            transformers.append((
                'num',
                'passthrough',
                numeric_features
            ))
        
        # 3. Boolean features (ê·¸ëŒ€ë¡œ)
        transformers.append((
            'bool',
            'passthrough',
            boolean_features
        ))
        
        # 4. High cardinality (ê·¸ëŒ€ë¡œ)
        transformers.append((
            'high_card',
            'passthrough',
            high_cardinality_features
        ))
        
        self.preprocessor = ColumnTransformer(
            transformers=transformers,
            remainder='drop'
        )
        
    def _encode_ordinal_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """Ordinal feature ì¸ì½”ë”©"""
        df = df.copy()
        if 'days_until_departure_bucket' in df.columns:
            if df['days_until_departure_bucket'].dtype == object:
                df['days_until_departure_bucket'] = df['days_until_departure_bucket'].map(
                    self.ordinal_mapping
                )
        return df
    
    def fit(self, X: pd.DataFrame, y=None):
        """ì „ì²˜ë¦¬ê¸°ë¥¼ í•™ìŠµ ë°ì´í„°ì— fit"""
        X_processed = self._encode_ordinal_features(X)
        self.preprocessor.fit(X_processed)
        return self
    
    def transform(self, X: pd.DataFrame) -> np.ndarray:
        """ë°ì´í„°ë¥¼ ì „ì²˜ë¦¬í•˜ì—¬ ë³€í™˜"""
        X_processed = self._encode_ordinal_features(X)
        X_transformed = self.preprocessor.transform(X_processed)
        return X_transformed
    
    def fit_transform(self, X: pd.DataFrame, y=None) -> np.ndarray:
        """fitê³¼ transformì„ í•œ ë²ˆì— ìˆ˜í–‰"""
        return self.fit(X, y).transform(X)
    
    def get_feature_names_out(self):
        """ë³€í™˜ í›„ feature ì´ë¦„ ë°˜í™˜"""
        try:
            return list(self.preprocessor.get_feature_names_out())
        except AttributeError:
            # Fallback for older scikit-learn versions
            return ['feature_' + str(i) for i in range(self.preprocessor.n_features_in_)]

def upload_df_to_s3(df: pd.DataFrame, s3_uri: str, filename: str):
    """
    DataFrameì„ CSVë¡œ ë³€í™˜í•˜ì—¬ S3ì— ì—…ë¡œë“œ
    """
    if not s3_uri.startswith("s3://"):
        raise ValueError("output_data_s3_uri must start with 's3://'")

    s3_path = s3_uri.replace("s3://", "")
    bucket = s3_path.split("/")[0]
    prefix = "/".join(s3_path.split("/")[1:]).rstrip("/")

    csv_buffer = StringIO()
    df.to_csv(csv_buffer, index=False)

    s3 = boto3.client("s3")
    s3.put_object(
        Bucket=bucket,
        Key=f"{prefix}/{filename}",
        Body=csv_buffer.getvalue()
    )

    print(f"ğŸ“¤ S3 ì €ì¥ ì™„ë£Œ: s3://{bucket}/{prefix}/{filename}")

def detect_outliers_iqr(data, column, multiplier=1.5):
    """
    IQR ë°©ë²•ìœ¼ë¡œ outlier íƒì§€
    
    Args:
        data: DataFrame
        column: ì»¬ëŸ¼ëª…
        multiplier: IQR ë°°ìˆ˜ (ê¸°ë³¸ 1.5, ë” ì—„ê²©í•˜ê²ŒëŠ” 3.0)
    
    Returns:
        lower_bound, upper_bound, outlier_mask
    """
    Q1 = data[column].quantile(0.25)
    Q3 = data[column].quantile(0.75)
    IQR = Q3 - Q1
    
    lower_bound = Q1 - multiplier * IQR
    upper_bound = Q3 + multiplier * IQR
    
    outlier_mask = (data[column] < lower_bound) | (data[column] > upper_bound)
    
    return lower_bound, upper_bound, outlier_mask


def preprocess(input_data_s3_uri: str, output_data_s3_uri: str, experiment_name="main_experiment", run_name="run-01") -> tuple:
    """
    í•­ê³µê¶Œ ê°€ê²© ì˜ˆì¸¡ ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜
    
    Args:
        input_data_s3_uri: S3ì— ì €ì¥ëœ ì›ë³¸ ë°ì´í„° ê²½ë¡œ (s3://bucket/path/to/file.csv)
        experiment_name: MLflow experiment ì´ë¦„
        run_name: MLflow run ì´ë¦„
    
    Returns:
        X_train, y_train, X_val, y_val, X_test, y_test, featurizer_model, run_id
    """
    
    # Enable autologging in MLflow
    mlflow.set_tracking_uri(os.environ['MLFLOW_TRACKING_ARN'])    
    mlflow.set_experiment(experiment_name)
    with mlflow.start_run(run_name=run_name) as run:
        run_id = run.info.run_id
        print(run)
        with mlflow.start_run(run_name="DataPreprocessing", nested=True):    
            mlflow.autolog()         
        
            print(f"âœ… MLflow Run ì‹œì‘: {run_id}")
            print(f"   Experiment: {experiment_name}")
            print(f"   Run Name: {run_name}")
            
            # 1. ë°ì´í„° ë¡œë“œ from S3
            print(f"\nğŸ“¥ ë°ì´í„° ë¡œë“œ ì¤‘: {input_data_s3_uri}")
            
            # S3ì—ì„œ ë°ì´í„° ì½ê¸°
            s3 = boto3.client('s3')
            
            # S3 URI íŒŒì‹±
            if input_data_s3_uri.startswith('s3://'):
                s3_path = input_data_s3_uri.replace('s3://', '')
                bucket_name = s3_path.split('/')[0]
                object_key = '/'.join(s3_path.split('/')[1:])
            else:
                raise ValueError("input_data_s3_uri must start with 's3://'")
            
            # S3ì—ì„œ íŒŒì¼ ì½ê¸°
            # obj = s3.get_object(Bucket=bucket_name, Key=object_key)
            # df_raw = pd.read_csv(StringIO(obj['Body'].read().decode('utf-8')))
            df_raw = pd.read_csv(input_data_s3_uri)
            
            print(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {df_raw.shape}")
            mlflow.log_param("raw_data_shape", str(df_raw.shape))
            mlflow.log_param("input_s3_uri", input_data_s3_uri)
            
            # 2. ì¤‘ë³µ ë°ì´í„° ì œê±°
            df_raw_before = df_raw.shape[0]
            df_raw = df_raw.drop_duplicates()
            df_raw = df_raw.reset_index(drop=True)
            df_raw_after = df_raw.shape[0]
            removed = df_raw_before - df_raw_after
            
            print(f"\nâœ… ì¤‘ë³µ ì œê±° ì™„ë£Œ")
            print(f"  - ì œê±° ì „: {df_raw_before:,}ê°œ")
            print(f"  - ì œê±° í›„: {df_raw_after:,}ê°œ")
            print(f"  - ì œê±°ëœ ë°ì´í„°: {removed:,}ê°œ")
            
            mlflow.log_metric("duplicates_removed", removed)
            mlflow.log_metric("data_after_dedup", df_raw_after)
            
            # 3. Outlier ì²˜ë¦¬ (Clipping ë°©ë²• ì‚¬ìš©)
            OUTLIER_METHOD = 'clip'  # 'remove', 'clip', 'log', 'none' ì¤‘ ì„ íƒ
            
            lower, upper, outlier_mask = detect_outliers_iqr(df_raw, 'Fare', multiplier=1.5)
            n_outliers = outlier_mask.sum()
            outlier_pct = (n_outliers / len(df_raw)) * 100
            
            print(f"\nğŸ” Outlier ë¶„ì„ (IQR ë°©ë²•):")
            print(f"  - Lower bound: â‚¹{lower:,.0f}")
            print(f"  - Upper bound: â‚¹{upper:,.0f}")
            print(f"  - Outliers: {n_outliers:,}ê°œ ({outlier_pct:.2f}%)")
            
            mlflow.log_param("outlier_method", OUTLIER_METHOD)
            mlflow.log_metric("outlier_lower_bound", lower)
            mlflow.log_metric("outlier_upper_bound", upper)
            mlflow.log_metric("n_outliers", n_outliers)
            mlflow.log_metric("outlier_percentage", outlier_pct)
            
            df_processed = df_raw.copy()
            
            if OUTLIER_METHOD == 'clip':
                # Outlier Clipping
                df_processed['Fare'] = df_processed['Fare'].clip(lower=lower, upper=upper)
                print(f"âœ… Outlier Clipping ì™„ë£Œ")
                print(f"  - {n_outliers:,}ê°œì˜ ê°’ì´ ê²½ê³„ê°’ìœ¼ë¡œ ëŒ€ì²´ë˜ì—ˆìŠµë‹ˆë‹¤")
            # 4. Train/Validation/Test Split (70/10/20)
            print("Split dataset (Train/Val/Test) ...")

            df_train_raw, df_temp_raw = train_test_split(
                df_processed, test_size=0.3, random_state=42
            )
            df_val_raw, df_test_raw = train_test_split(
                df_temp_raw, test_size=2/3, random_state=42
            )

            print(f"???????????? ?????? ????? (Train 70% / Validation 10% / Test 20%)")
            print(f"  - Train: {df_train_raw.shape[0]:,}???")
            print(f"  - Validation: {df_val_raw.shape[0]:,}???")
            print(f"  - Test: {df_test_raw.shape[0]:,}???")

            mlflow.log_metric("train_size", df_train_raw.shape[0])
            mlflow.log_metric("val_size", df_val_raw.shape[0])
            mlflow.log_metric("test_size", df_test_raw.shape[0])

            # 5. Feature Engineering
            print("Feature engineering...")

            apply_log = True
            engineer = FlightFeatureEngineer(apply_log_to_target=apply_log)
            df_train_features = engineer.transform(df_train_raw)
            df_val_features = engineer.transform(df_val_raw)
            df_test_features = engineer.transform(df_test_raw)

            def _dedup_features(df_features, label):
                duplicates = df_features.duplicated().sum()
                if duplicates > 0:
                    df_before = df_features.shape[0]
                    df_features = df_features.drop_duplicates().reset_index(drop=True)
                    removed = df_before - df_features.shape[0]
                    print(f"  - Feature Engineering {label} ???????? ?????: {removed:,}???")
                    mlflow.log_metric(f"duplicates_removed_after_fe_{label}", removed)
                return df_features

            df_train_features = _dedup_features(df_train_features, "train")
            df_val_features = _dedup_features(df_val_features, "val")
            df_test_features = _dedup_features(df_test_features, "test")

            df_features = pd.concat(
                [df_train_features, df_val_features, df_test_features],
                ignore_index=True,
            )

            print(f"??Feature Engineering ?????")
            print(f"  - Feature ??????: {df_train_features.shape[1]}")

            mlflow.log_param("feature_count", df_train_features.shape[1])
            mlflow.log_param("apply_log_to_target", apply_log)

            target_col = 'price'
            drop_cols = [col for col in ['price', 'price_original'] if col in df_train_features.columns]

            X_train = df_train_features.drop(columns=drop_cols)
            y_train = df_train_features[target_col]
            X_val = df_val_features.drop(columns=drop_cols)
            y_val = df_val_features[target_col]
            X_test = df_test_features.drop(columns=drop_cols)
            y_test = df_test_features[target_col]

            # ===== CSV ??????? ????????????? =====
            train_df = X_train.copy()
            train_df["price"] = y_train.values

            val_df = X_val.copy()
            val_df["price"] = y_val.values

            test_df = X_test.copy()
            test_df["price"] = y_test.values

            # ===== S3 ????=====
            upload_df_to_s3(train_df, output_data_s3_uri, "train.csv")
            upload_df_to_s3(val_df, output_data_s3_uri, "validation.csv")
            upload_df_to_s3(test_df, output_data_s3_uri, "test.csv")

            mlflow.log_param("output_data_s3_uri", output_data_s3_uri)

            # 6. ML Preprocessing
            print("Processing...")
            print(f"\nğŸ”§ ML Preprocessing ì‹œì‘...")
            
            featurizer_model = FlightPricePreprocessor(scale_numeric=True)
            
            # Train ë°ì´í„°ë¡œ fit & transform
            X_train_processed = featurizer_model.fit_transform(X_train)
            
            # Validation & Test ë°ì´í„° transform
            X_val_processed = featurizer_model.transform(X_val)
            X_test_processed = featurizer_model.transform(X_test)
            
            print(f"âœ… ML Preprocessing ì™„ë£Œ")
            print(f"  - ì›ë³¸ feature ìˆ˜: {X_train.shape[1]}")
            print(f"  - ë³€í™˜ í›„ feature ìˆ˜: {X_train_processed.shape[1]}")
            
            mlflow.log_metric("original_feature_count", X_train.shape[1])
            mlflow.log_metric("transformed_feature_count", X_train_processed.shape[1])
            
            # 7. Featurizer ëª¨ë¸ ì €ì¥ (MLflow)
            print(f"\nğŸ’¾ Featurizer ëª¨ë¸ ì €ì¥ ì¤‘...")
            safe_name = "".join(
                ch if ch.isalnum() else "-" for ch in f"{experiment_name}-featurizer"
            ).strip("-")
            safe_name = safe_name[:57]
            featurizer_transformer = featurizer_model.preprocessor
            mlflow.sklearn.log_model(
                featurizer_transformer,
                "featurizer",
                registered_model_name=safe_name
            )
            print(f"âœ… Featurizer ëª¨ë¸ ì €ì¥ ì™„ë£Œ")

            # ===============================
            # SageMaker ë°°í¬ìš© Featurizer ëª¨ë¸ ì €ì¥
            # ===============================
            model_file_path = "/opt/ml/model/sklearn_model.joblib"
            os.makedirs(os.path.dirname(model_file_path), exist_ok=True)

            joblib.dump(featurizer_transformer, model_file_path)
            mlflow.log_artifact(model_file_path, artifact_path="model")

            # ===== ìµœì¢… ê²°ê³¼ ë°ì´í„°ì…‹ S3 ì €ì¥ =====
            upload_df_to_s3(
                df_features,
                output_data_s3_uri,
                "final_dataset.csv"
            )

            # 8. í†µê³„ ë¡œê¹…
            mlflow.log_metric("train_mean_price", float(y_train.mean()))
            mlflow.log_metric("val_mean_price", float(y_val.mean()))
            mlflow.log_metric("test_mean_price", float(y_test.mean()))
            mlflow.log_metric("train_std_price", float(y_train.std()))
            
            # 9. ì™„ë£Œ ìš”ì•½
            print(f"\n" + "="*70)
            print(f"ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ")
            print(f"="*70)
            print(f"\nğŸ“Š ìµœì¢… ë°ì´í„°ì…‹:")
            print(f"  - Train: {X_train_processed.shape}")
            print(f"  - Validation: {X_val_processed.shape}")
            print(f"  - Test: {X_test_processed.shape}")
            print(f"\nğŸ’° Target í†µê³„:")
            print(f"  - Train í‰ê· : â‚¹{y_train.mean():,.2f}")
            print(f"  - Validation í‰ê· : â‚¹{y_val.mean():,.2f}")
            print(f"  - Test í‰ê· : â‚¹{y_test.mean():,.2f}")
            print(f"\nâœ… MLflow Run ID: {run_id}")
            
    # numpy arrayë¡œ ë³€í™˜ ë° ë°˜í™˜
    return (
        X_train_processed,
        y_train.values,
        X_val_processed,
        y_val.values,
        X_test_processed,
        y_test.values,
        featurizer_transformer,
        run_id
    )

