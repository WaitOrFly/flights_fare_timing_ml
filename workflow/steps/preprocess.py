"""
Flight Price Preprocessing Module

ì´ ëª¨ë“ˆì€ í•­ê³µê¶Œ ê°€ê²© ì˜ˆì¸¡ ë°ì´í„° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ì„ í¬í•¨í•©ë‹ˆë‹¤.
"""
import os
import joblib

import pandas as pd
import numpy as np
import hashlib
import warnings
import boto3
from io import StringIO

from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
import mlflow
import mlflow.sklearn

from .feature_engineer import FlightFeatureEngineer, RouteHashEncoder, FlightPricePreprocessor


warnings.filterwarnings('ignore')

def upload_df_to_s3(df: pd.DataFrame, s3_uri: str, filename: str):
    """
    DataFrameì„ CSVë¡œ ë³€í™˜í•˜ì—¬ S3ì— ì—…ë¡œë“œ
    """
    if not s3_uri.startswith("s3://"):
        raise ValueError("output_data_s3_uri must start with 's3://'")

    s3_path = s3_uri.replace("s3://", "")
    bucket = s3_path.split("/")[0]
    prefix = "/".join(s3_path.split("/")[1:]).rstrip("/")

    csv_buffer = StringIO()
    df.to_csv(csv_buffer, index=False)

    s3 = boto3.client("s3")
    s3.put_object(
        Bucket=bucket,
        Key=f"{prefix}/{filename}",
        Body=csv_buffer.getvalue()
    )

    print(f"ğŸ“¤ S3 ì €ì¥ ì™„ë£Œ: s3://{bucket}/{prefix}/{filename}")

def upload_file_to_s3(local_path: str, s3_uri: str):
    """
    ë¡œì»¬ íŒŒì¼ì„ S3ì— ì—…ë¡œë“œ (joblib, bin ë“±)
    """
    if not s3_uri.startswith("s3://"):
        raise ValueError("s3_uri must start with 's3://'")

    s3_path = s3_uri.replace("s3://", "")
    bucket = s3_path.split("/")[0]
    key = "/".join(s3_path.split("/")[1:])

    s3 = boto3.client("s3")
    s3.upload_file(local_path, bucket, key)

    print(f"ğŸ“¤ S3 ì €ì¥ ì™„ë£Œ: {s3_uri}")

def detect_outliers_iqr(data, column, multiplier=1.5):
    """
    IQR ë°©ë²•ìœ¼ë¡œ outlier íƒì§€
    """
    Q1 = data[column].quantile(0.25)
    Q3 = data[column].quantile(0.75)
    IQR = Q3 - Q1

    lower_bound = Q1 - multiplier * IQR
    upper_bound = Q3 + multiplier * IQR

    outlier_mask = (data[column] < lower_bound) | (data[column] > upper_bound)

    return lower_bound, upper_bound, outlier_mask

def time_stratified_split_by_crawl(
    df: pd.DataFrame,
    time_col: str = "Crawl Timestamp",
    target_col: str = "Fare",
    train_ratio: float = 0.7,
    val_ratio: float = 0.1,
    n_time_blocks: int = 10,
    n_price_bins: int = 5,
    random_state: int = 42,
):
    """
    ì‹œê°„ ìˆœì„œë¥¼ ìœ ì§€í•˜ë©´ì„œ target(Fare) ë¶„í¬ë¥¼ ì•ˆì •í™”í•˜ëŠ” split
    - train:test:val = 7:2:1
    - ì‹œê°„ block ë‹¨ìœ„ë¡œ ë‚˜ëˆˆ ë’¤, block ë‚´ë¶€ì—ì„œ ê°€ê²© ë¶„ìœ„ìˆ˜ ê¸°ë°˜ ë¶„í• 
    - ë¯¸ë˜ ë°ì´í„° ëˆ„ìˆ˜ ì—†ìŒ
    """

    assert train_ratio + val_ratio < 1.0, "train_ratio + val_ratio must be < 1"

    df = df.copy()
    df["crawl_datetime"] = pd.to_datetime(
        df[time_col], utc=True
    ).dt.tz_localize(None)

    # 1ï¸âƒ£ ì‹œê°„ ê¸°ì¤€ ì •ë ¬
    df = df.sort_values("crawl_datetime").reset_index(drop=True)

    # 2ï¸âƒ£ ì‹œê°„ block ìƒì„± (ìˆœì„œ ìœ ì§€)
    df["time_block"] = pd.qcut(
        df.index,
        q=n_time_blocks,
        labels=False,
        duplicates="drop"
    )

    train_parts, val_parts, test_parts = [], [], []

    # 3ï¸âƒ£ block ë‹¨ìœ„ ë¶„í• 
    for _, block_df in df.groupby("time_block", sort=False):
        block_df = block_df.copy()

        # block ë‚´ë¶€ ê°€ê²© ë¶„ìœ„ìˆ˜ bin
        block_df["price_bin"] = pd.qcut(
            block_df[target_col],
            q=min(n_price_bins, block_df[target_col].nunique()),
            duplicates="drop"
        )

        # block ë‚´ë¶€ ì…”í”Œ (ì‹œê°„ block ì•ˆì—ì„œë§Œ)
        block_df = block_df.sample(frac=1, random_state=random_state)

        n = len(block_df)
        train_end = int(n * train_ratio)
        val_end = int(n * (train_ratio + val_ratio))

        train_parts.append(block_df.iloc[:train_end])
        val_parts.append(block_df.iloc[train_end:val_end])
        test_parts.append(block_df.iloc[val_end:])

    # 4ï¸âƒ£ ìµœì¢… í•©ì¹˜ê¸° + ì‹œê°„ ìˆœ ë³µì›
    df_train = pd.concat(train_parts).sort_values("crawl_datetime").reset_index(drop=True)
    df_val   = pd.concat(val_parts).sort_values("crawl_datetime").reset_index(drop=True)
    df_test  = pd.concat(test_parts).sort_values("crawl_datetime").reset_index(drop=True)

    return df_train, df_val, df_test



def preprocess(
    input_data_s3_uri: str,
    output_data_s3_uri: str,
    experiment_name="main_experiment",
    run_name="run-01"
) -> tuple:

    # Enable autologging in MLflow
    mlflow.set_tracking_uri(os.environ['MLFLOW_TRACKING_ARN'])
    mlflow.set_experiment(experiment_name)

    with mlflow.start_run(run_name=run_name) as run:
        run_id = run.info.run_id
        print(run)

        with mlflow.start_run(run_name="DataPreprocessing", nested=True):
            mlflow.autolog()

            print(f"âœ… MLflow Run ì‹œì‘: {run_id}")
            print(f"   Experiment: {experiment_name}")
            print(f"   Run Name: {run_name}")

            # 1. ë°ì´í„° ë¡œë“œ
            print(f"\nğŸ“¥ ë°ì´í„° ë¡œë“œ ì¤‘: {input_data_s3_uri}")

            # (ì°¸ê³ ) ì•„ë˜ s3 get_object ì½”ë“œëŠ” ìœ ì§€. í˜„ì¬ëŠ” s3fsë¡œ ì½ëŠ” ë°©ì‹ ì‚¬ìš© ì¤‘.
            if not input_data_s3_uri.startswith('s3://'):
                raise ValueError("input_data_s3_uri must start with 's3://'")

            df_raw = pd.read_csv(input_data_s3_uri)

            print(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {df_raw.shape}")
            mlflow.log_param("raw_data_shape", str(df_raw.shape))
            mlflow.log_param("input_s3_uri", input_data_s3_uri)

            # 2. ì¤‘ë³µ ì œê±°
            df_raw_before = df_raw.shape[0]
            df_raw = df_raw.drop_duplicates().reset_index(drop=True)
            df_raw_after = df_raw.shape[0]
            
            removed = df_raw_before - df_raw_after

            print(f"\nâœ… ì¤‘ë³µ ì œê±° ì™„ë£Œ")
            print(f"  - ì œê±° ì „: {df_raw_before:,}ê°œ")
            print(f"  - ì œê±° í›„: {df_raw_after:,}ê°œ")
            print(f"  - ì œê±°ëœ ë°ì´í„°: {removed:,}ê°œ")

            mlflow.log_metric("duplicates_removed", removed)
            mlflow.log_metric("data_after_dedup", df_raw_after)

            # 3. Outlier ì²˜ë¦¬ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
            OUTLIER_METHOD = 'clip'  # 'remove', 'clip', 'log', 'none' ì¤‘ ì„ íƒ
            
            lower, upper, outlier_mask = detect_outliers_iqr(df_raw, 'Fare', multiplier=1.5)
            n_outliers = int(outlier_mask.sum())
            outlier_pct = (n_outliers / len(df_raw)) * 100

            print(f"\nğŸ” Outlier ë¶„ì„ (IQR ë°©ë²•):")
            print(f"  - Lower bound: â‚¹{lower:,.0f}")
            print(f"  - Upper bound: â‚¹{upper:,.0f}")
            print(f"  - Outliers: {n_outliers:,}ê°œ ({outlier_pct:.2f}%)")

            mlflow.log_param("outlier_method", OUTLIER_METHOD)
            mlflow.log_metric("outlier_lower_bound", float(lower))
            mlflow.log_metric("outlier_upper_bound", float(upper))
            mlflow.log_metric("n_outliers", n_outliers)
            mlflow.log_metric("outlier_percentage", float(outlier_pct))

            df_processed = df_raw.copy()

            if OUTLIER_METHOD == 'clip':
                df_processed['Fare'] = df_processed['Fare'].clip(lower=lower, upper=upper)
                print(f"âœ… Outlier Clipping ì™„ë£Œ")
                print(f"  - {n_outliers:,}ê°œì˜ ê°’ì´ ê²½ê³„ê°’ìœ¼ë¡œ ëŒ€ì²´ë˜ì—ˆìŠµë‹ˆë‹¤")

            # Route-level stats for inference (mean fare per route).
            route_stats = (
                df_processed.groupby(["Source", "Destination"], as_index=False)["Fare"]
                .mean()
            )
            route_stats["Crawl Timestamp"] = df_processed["Crawl Timestamp"].min()
            upload_df_to_s3(route_stats, output_data_s3_uri, "route_stats.csv")

            # 4. ì‹œê°„ ê¸°ë°˜ + ê°€ê²© ì•ˆì •í™” split (ëˆ„ìˆ˜ ë°©ì§€)
            print(f"\nğŸ“Š (Leak-Free) crawl_timestamp ê¸°ì¤€ ë°ì´í„°ì…‹ ë¶„í•  ì¤‘...")

            df_train_raw, df_val_raw, df_test_raw = time_stratified_split_by_crawl(
                df_processed,
                train_ratio=0.7,
                val_ratio=0.1
            )

            print(f"âœ… ë°ì´í„°ì…‹ ë¶„í•  ì™„ë£Œ (Train 70% / Validation 10% / Test 20%)")
            print(f"  - Train: {df_train_raw.shape[0]:,}ê°œ")
            print(f"  - Validation: {df_val_raw.shape[0]:,}ê°œ")
            print(f"  - Test: {df_test_raw.shape[0]:,}ê°œ")

            mlflow.log_metric("train_size", int(df_train_raw.shape[0]))
            mlflow.log_metric("val_size", int(df_val_raw.shape[0]))
            mlflow.log_metric("test_size", int(df_test_raw.shape[0]))

            # 5. Feature Engineering (splitë³„ + ì•ˆì „í•œ price_trend)
            print(f"\nâš™ï¸ Feature Engineering ì‹œì‘ (splitë³„, ê³¼ê±° ë°ì´í„°ë§Œ ì°¸ì¡°)...")
            
            APPLY_LOG_TARGET = True  # íƒ€ê¹ƒì€ í•­ìƒ log ì‚¬ìš©

            engineer = FlightFeatureEngineer(apply_log_to_target=APPLY_LOG_TARGET)

            # Train: train ë‚´ë¶€ ê³¼ê±°ë§Œ ì‚¬ìš© (historical_df=None â†’ df ë‚´ë¶€ shift(1) ê¸°ë°˜)
            train_features = engineer.transform(df_train_raw, historical_df=None)

            # Val: ê³¼ê±° = train
            val_features = engineer.transform(df_val_raw, historical_df=df_train_raw)

            # Test: ê³¼ê±° = train + val
            test_features = engineer.transform(df_test_raw, historical_df=pd.concat([df_train_raw, df_val_raw], axis=0))

            # (ì°¸ê³ ) ìµœì¢… dataset ì €ì¥ìš© (schema ë™ì¼, ëˆ„ìˆ˜ ì œê±°ëœ ë²„ì „)
            df_features_all = pd.concat([train_features, val_features, test_features], axis=0).reset_index(drop=True)

            # ì¤‘ë³µ ì œê±° (ê¸°ì¡´ ì˜ë„ ìœ ì§€)
            duplicates_after_fe = int(df_features_all.duplicated().sum())
            if duplicates_after_fe > 0:
                before = df_features_all.shape[0]
                df_features_all = df_features_all.drop_duplicates().reset_index(drop=True)
                after = df_features_all.shape[0]
                removed_fe = before - after
                print(f"  - Feature Engineering í›„ ì¤‘ë³µ ì œê±°: {removed_fe:,}ê°œ")
                mlflow.log_metric("duplicates_removed_after_fe", int(removed_fe))

            print(f"âœ… Feature Engineering ì™„ë£Œ")
            print(f"  - Feature ê°œìˆ˜: {df_features_all.shape[1]}")

            mlflow.log_param("feature_count", int(df_features_all.shape[1]))
            mlflow.log_param("apply_log_to_target", bool(APPLY_LOG_TARGET))

            # 6. Targetê³¼ features ë¶„ë¦¬ (splitë³„)
            target_col = 'price'

            drop_cols = ['price', 'price_original'] if 'price_original' in train_features.columns else ['price']

            X_train = train_features.drop(drop_cols, axis=1)
            y_train = train_features[target_col]

            X_val = val_features.drop(drop_cols, axis=1)
            y_val = val_features[target_col]

            X_test = test_features.drop(drop_cols, axis=1)
            y_test = test_features[target_col]

            # âœ… í‰ê°€ìš© ì›ë³¸ íƒ€ê¹ƒì€ í•­ìƒ í™•ë³´
            if "price_original" in val_features.columns:
                y_val_original = val_features["price_original"]
            else:
                # logë¥¼ ì•ˆ ì“°ëŠ” ê²½ìš°ì—” price ìì²´ê°€ ì›ë³¸
                y_val_original = val_features["price"]

            if "price_original" in test_features.columns:
                y_test_original = test_features["price_original"]
            else:
                y_test_original = test_features["price"]
                
            # ===== CSV ì €ì¥ìš© ë°ì´í„° êµ¬ì„± & S3 ì €ì¥ (ê¸°ì¡´ íë¦„ ìœ ì§€) =====
            train_df = X_train.copy()
            train_df["price"] = y_train.values

            val_df = X_val.copy()
            val_df["price"] = y_val.values

            test_df = X_test.copy()
            test_df["price"] = y_test.values

            upload_df_to_s3(train_df, output_data_s3_uri, "train.csv")
            upload_df_to_s3(val_df, output_data_s3_uri, "validation.csv")
            upload_df_to_s3(test_df, output_data_s3_uri, "test.csv")

            mlflow.log_param("output_data_s3_uri", output_data_s3_uri)

            # 7. ML Preprocessing (ì¸ì½”ë”© & ìŠ¤ì¼€ì¼ë§) - trainë§Œ fit
            print(f"\nğŸ”§ ML Preprocessing ì‹œì‘...")

            featurizer_model = FlightPricePreprocessor(scale_numeric=True)

            X_train_processed = featurizer_model.fit_transform(X_train)
            X_val_processed = featurizer_model.transform(X_val)
            X_test_processed = featurizer_model.transform(X_test)

            print(f"âœ… ML Preprocessing ì™„ë£Œ")
            print(f"  - ì›ë³¸ feature ìˆ˜: {X_train.shape[1]}")
            print(f"  - ë³€í™˜ í›„ feature ìˆ˜: {X_train_processed.shape[1]}")

            mlflow.log_metric("original_feature_count", int(X_train.shape[1]))
            mlflow.log_metric("transformed_feature_count", int(X_train_processed.shape[1]))

            # 8. Featurizer ëª¨ë¸ ì €ì¥ (MLflow) - ê¸°ì¡´ í˜•íƒœ ìœ ì§€
            print(f"\nğŸ’¾ Featurizer ëª¨ë¸ ì €ì¥ ì¤‘...")
            mlflow.sklearn.log_model(
                featurizer_model,
                artifact_path="featurizer"
            )
            print(f"âœ… Featurizer ëª¨ë¸ ì €ì¥ ì™„ë£Œ")

            # SageMaker ë°°í¬ìš© Featurizer ëª¨ë¸ ì €ì¥
            featurizer_s3_uri = f"{output_data_s3_uri}/featurizer/featurizer.joblib"
            local_featurizer_path = "/tmp/featurizer.joblib"

            # 1ï¸âƒ£ ë¡œì»¬ ì €ì¥
            joblib.dump(featurizer_model, local_featurizer_path)

            # 2ï¸âƒ£ S3 ì—…ë¡œë“œ (ê³µí†µ ìœ í‹¸ ì‚¬ìš©)
            upload_file_to_s3(local_featurizer_path, featurizer_s3_uri)

            # 3ï¸âƒ£ MLflowì—ëŠ” ë¡œì»¬ íŒŒì¼ë§Œ ê¸°ë¡
            mlflow.log_artifact(local_featurizer_path, artifact_path="featurizer")


            # ===== ìµœì¢… ê²°ê³¼ ë°ì´í„°ì…‹ S3 ì €ì¥ (ëˆ„ìˆ˜ ì œê±° ë²„ì „) =====
            upload_df_to_s3(
                df_features_all,
                output_data_s3_uri,
                "final_dataset.csv"
            )

            # 9. í†µê³„ ë¡œê¹…
            mlflow.log_metric("train_mean_log_price", float(y_train.mean()))
            mlflow.log_metric("val_mean_log_price", float(y_val.mean()))
            mlflow.log_metric("test_mean_log_price", float(y_test.mean()))

            mlflow.log_metric(
                "train_mean_price_original",
                float(train_features["price_original"].mean())
            )
            
            # 10. ì™„ë£Œ ìš”ì•½
            print(f"\n" + "=" * 70)
            print(f"ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ (Leak-Free)")
            print(f"=" * 70)
            print(f"\nğŸ“Š ìµœì¢… ë°ì´í„°ì…‹:")
            print(f"  - Train: {X_train_processed.shape}")
            print(f"  - Validation: {X_val_processed.shape}")
            print(f"  - Test: {X_test_processed.shape}")
            print(f"ğŸ’° Target í†µê³„ (log-space):")
            print(f"  - Train í‰ê· (log): {y_train.mean():.4f}")
            print(f"  - Validation í‰ê· (log): {y_val.mean():.4f}")
            print(f"  - Test í‰ê· (log): {y_test.mean():.4f}")
            print(f"\nğŸ’° Target í†µê³„ (INR, original scale):")
            print(f"  - Train í‰ê· : â‚¹{train_features['price_original'].mean():,.2f}")
            print(f"  - Validation í‰ê· : â‚¹{val_features['price_original'].mean():,.2f}")
            print(f"  - Test í‰ê· : â‚¹{test_features['price_original'].mean():,.2f}")
            print(f"\nâœ… MLflow Run ID: {run_id}")

    return (
        X_train_processed,
        y_train.values,
        X_val_processed,
        y_val.values,
        y_val_original.values,
        X_test_processed,
        y_test.values,
        y_test_original.values,
        run_id
    )
